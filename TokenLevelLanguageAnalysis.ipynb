{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TokenLevelLanguageAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Or0K1YRD8S5o",
        "gLeUZrIx8mQH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or0K1YRD8S5o",
        "colab_type": "text"
      },
      "source": [
        "# Download Vectors + Install Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlLW93g78QsC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8a19b88f-add4-4f83-de4c-8b537b506255"
      },
      "source": [
        "!wget \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ceb.300.vec.gz\"\n",
        "!wget \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.bcl.300.vec.gz\"\n",
        "!wget \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\"\n",
        "!wget \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ilo.300.vec.gz\"\n",
        "!wget \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pam.300.vec.gz\"\n",
        "!wget \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tl.300.vec.gz\"\n",
        "!wget \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.war.300.vec.gz\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-16 16:58:56--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ceb.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1220456596 (1.1G) [binary/octet-stream]\n",
            "Saving to: ‘cc.ceb.300.vec.gz’\n",
            "\n",
            "cc.ceb.300.vec.gz   100%[===================>]   1.14G  20.8MB/s    in 61s     \n",
            "\n",
            "2020-07-16 16:59:57 (19.1 MB/s) - ‘cc.ceb.300.vec.gz’ saved [1220456596/1220456596]\n",
            "\n",
            "--2020-07-16 16:59:59--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.bcl.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65851421 (63M) [binary/octet-stream]\n",
            "Saving to: ‘cc.bcl.300.vec.gz’\n",
            "\n",
            "cc.bcl.300.vec.gz   100%[===================>]  62.80M  17.4MB/s    in 4.1s    \n",
            "\n",
            "2020-07-16 17:00:04 (15.4 MB/s) - ‘cc.bcl.300.vec.gz’ saved [65851421/65851421]\n",
            "\n",
            "--2020-07-16 17:00:05--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz’\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G  22.4MB/s    in 58s     \n",
            "\n",
            "2020-07-16 17:01:03 (21.9 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n",
            "\n",
            "--2020-07-16 17:01:04--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ilo.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 134838043 (129M) [binary/octet-stream]\n",
            "Saving to: ‘cc.ilo.300.vec.gz’\n",
            "\n",
            "cc.ilo.300.vec.gz   100%[===================>] 128.59M  22.2MB/s    in 6.5s    \n",
            "\n",
            "2020-07-16 17:01:11 (19.9 MB/s) - ‘cc.ilo.300.vec.gz’ saved [134838043/134838043]\n",
            "\n",
            "--2020-07-16 17:01:12--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pam.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 96329945 (92M) [binary/octet-stream]\n",
            "Saving to: ‘cc.pam.300.vec.gz’\n",
            "\n",
            "cc.pam.300.vec.gz   100%[===================>]  91.87M  21.6MB/s    in 4.8s    \n",
            "\n",
            "2020-07-16 17:01:18 (19.0 MB/s) - ‘cc.pam.300.vec.gz’ saved [96329945/96329945]\n",
            "\n",
            "--2020-07-16 17:01:19--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tl.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 423421446 (404M) [binary/octet-stream]\n",
            "Saving to: ‘cc.tl.300.vec.gz’\n",
            "\n",
            "cc.tl.300.vec.gz    100%[===================>] 403.81M  22.1MB/s    in 19s     \n",
            "\n",
            "2020-07-16 17:01:39 (21.1 MB/s) - ‘cc.tl.300.vec.gz’ saved [423421446/423421446]\n",
            "\n",
            "--2020-07-16 17:01:39--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.war.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 573076954 (547M) [binary/octet-stream]\n",
            "Saving to: ‘cc.war.300.vec.gz’\n",
            "\n",
            "cc.war.300.vec.gz   100%[===================>] 546.53M  22.4MB/s    in 25s     \n",
            "\n",
            "2020-07-16 17:02:05 (21.7 MB/s) - ‘cc.war.300.vec.gz’ saved [573076954/573076954]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr1cXnbS8SJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip \"cc.ceb.300.vec.gz\"\n",
        "!gunzip \"cc.bcl.300.vec.gz\"\n",
        "!gunzip \"cc.en.300.vec.gz\"\n",
        "!gunzip \"cc.ilo.300.vec.gz\"\n",
        "!gunzip \"cc.pam.300.vec.gz\"\n",
        "!gunzip \"cc.tl.300.vec.gz\"\n",
        "!gunzip \"cc.war.300.vec.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-VWPHI6F4ae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc51d13-cc96-4049-e62a-15d21b4c979d"
      },
      "source": [
        "!pip install ftfy\n",
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d8/5e877ac5e827eaa41a7ea8c0dc1d3042e05d7e337604dc2aedb854e7b500/ftfy-5.7.tar.gz (58kB)\n",
            "\r\u001b[K     |█████▋                          | 10kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.7-cp36-none-any.whl size=44593 sha256=bca318d351623f4b627bb6667712e58c5b8328176fa3c25c92bd489d06a802fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/da/59/6c8925d571aacade638a0f515960c21c0887af1bfe31908fbf\n",
            "Successfully built ftfy\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-5.7\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (49.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3018693 sha256=f0388ebad77fed5ffb45c8886eaea596535e054348a5635ac742c5913e8bde7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCvoGCVu8bbJ",
        "colab_type": "text"
      },
      "source": [
        "# Loading, Cleaning, and Tokenizing Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guuQpz-DEc3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ftfy import fix_encoding\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import csv\n",
        "\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
        "        u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
        "        u\"\\U0001F600-\\U0001F64F\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U0001F1F2\"\n",
        "        u\"\\U0001F1F4\"\n",
        "        u\"\\U0001F620\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "\n",
        "hashtag_pattern = re.compile(u\"#\\w+\")\n",
        "\n",
        "userhandle_pattern = re.compile(u\"@(\\w){1,30}\")\n",
        "\n",
        "def fix_text(text):\n",
        "  # Emoji remover\n",
        "  text = emoji_pattern.sub(r'', text)\n",
        "  # Hashtag remover\n",
        "  text = hashtag_pattern.sub(r'', text)\n",
        "  # Userhandle remover\n",
        "  text = userhandle_pattern.sub(r'', text)\n",
        "  # Link remover\n",
        "  text = re.sub(r\"http\\S+\", r'', text)\n",
        "  # Punctation remover\n",
        "  text = re.sub(r'[.,\\/#!?$%\\^&\\*;:{}=\\-_`~()\"]', r'', text)\n",
        "  text = re.sub(r\"'\", r'', text)\n",
        "  # New line to space character\n",
        "  text = re.sub(r\"[\\r\\n]\", \" \", text)\n",
        "  # Reduce multiple space characters to only one\n",
        "  text = re.sub(r\"\\s\\s+\", \" \", text)\n",
        "  # Lowercase all characters\n",
        "  text = text.lower()\n",
        "\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAhGkCUcaae5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removed to hide Google Drive fiel structure\n",
        "instagram_file_path\n",
        "twitter_file_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONRMvD1QF1W9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f6c7504e-7cab-4bd0-8963-e514e56b2258"
      },
      "source": [
        "print(\"Loading Instagram posts...\")\n",
        "instagram_df = pd.read_csv(\n",
        "\tinstagram_file_path, \n",
        "\tencoding=\"utf-8\",\n",
        "\tquoting=csv.QUOTE_ALL)\n",
        "instagram_df.set_index(\"InstaPostID\", inplace=True)\n",
        "instagram_df = instagram_df[['UserID', 'Caption']]\n",
        "\n",
        "print(\"Loading Twitter posts...\")\n",
        "twitter_df = pd.read_csv(\n",
        "\ttwitter_file_path, \n",
        "\tencoding=\"utf-8\",\n",
        "\tquoting=csv.QUOTE_ALL)\n",
        "twitter_df.set_index(\"TweetID\", inplace=True)\n",
        "twitter_df = twitter_df[['UserID', 'Text', 'Language']]\n",
        "twitter_df['Text'] = twitter_df['Text'].fillna(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Instagram posts...\n",
            "Loading Twitter posts...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz88FnG0GDcK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "75772432-08c9-48b4-dcd9-c89d089f0207"
      },
      "source": [
        "print(\"INITIAL STATISTICS:\")\n",
        "print(\"TOTAL TWITTER POSTS: %d\" % len(twitter_df))\n",
        "print(\"TOTAL INSTAGRAM POSTS: %d\" % len(instagram_df))\n",
        "print(\"TOTAL INSTAGRAM POSTS W/O TEXT: %d\" % len(instagram_df[instagram_df.Caption.isna()]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INITIAL STATISTICS:\n",
            "TOTAL TWITTER POSTS: 4018628\n",
            "TOTAL INSTAGRAM POSTS: 195757\n",
            "TOTAL INSTAGRAM POSTS W/O TEXT: 17107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uOBDBgRGFgz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3159e4a2-404c-44ed-a717-d361735d5c92"
      },
      "source": [
        "print(\"Removing captions w/ no text...\")\n",
        "instagram_df = instagram_df[~instagram_df.Caption.isna()]\n",
        "print(\"done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removing captions w/ no text...\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pce6ILg_GHXL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3d3ebd28-3d74-45ba-c3f3-f31c3fdfc95c"
      },
      "source": [
        "print(\"Fixing text...\")\n",
        "instagram_df['Caption'] = instagram_df['Caption'].apply(fix_text)\n",
        "print(\"Instagram done.\")\n",
        "twitter_df['Text'] = twitter_df['Text'].apply(fix_text)\n",
        "print(\"Twitter done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fixing text...\n",
            "Instagram done.\n",
            "Twitter done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PwQxpxQHfe9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "4b4665c1-ea32-4e81-d655-dc3b9d9d03d1"
      },
      "source": [
        "import fasttext\n",
        "\n",
        "print(\"Extracting tokens...\")\n",
        "print(\"Instagram...\")\n",
        "instagram_captions = instagram_df['Caption'].tolist()\n",
        "instagram_tokens = []\n",
        "for post in instagram_captions:\n",
        "  tokens = fasttext.tokenize(post)\n",
        "  instagram_tokens.extend(tokens)\n",
        "total_instagram_tokens = len(instagram_tokens)\n",
        "total_instagram_vocabulary = len(set(instagram_tokens))\n",
        "print(\"Total Instagram tokens: %d\" % total_instagram_tokens)\n",
        "print(\"Total Instagram vocabulary: %d\"%  total_instagram_vocabulary)\n",
        "print(\"done.\")\n",
        "\n",
        "print(\"Twitter...\")\n",
        "twitter_captions = twitter_df['Text'].tolist()\n",
        "twitter_tokens = []\n",
        "for tweet in twitter_captions:\n",
        "  tokens = fasttext.tokenize(tweet)\n",
        "  twitter_tokens.extend(tokens)\n",
        "total_twitter_tokens = len(twitter_tokens)\n",
        "total_twitter_vocabulary = len(set(twitter_tokens))\n",
        "print(\"Total Twitter tokens: %d\" % total_twitter_tokens)\n",
        "print(\"Total Twitter vocabulary: %d\"%  total_twitter_vocabulary)\n",
        "print(\"done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting tokens...\n",
            "Instagram...\n",
            "Total Instagram tokens: 1786309\n",
            "Total Instagram vocabulary: 92030\n",
            "done.\n",
            "Twitter...\n",
            "Total Twitter tokens: 33451199\n",
            "Total Twitter vocabulary: 719927\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLeUZrIx8mQH",
        "colab_type": "text"
      },
      "source": [
        "# Preparing in Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc1l4DqZUEfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "\n",
        "def load_vectors(fname):\n",
        "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "    data = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        data[tokens[0]] = map(float, tokens[1:])\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGhGK_pZHZ1K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3094eb-8ab9-4691-eeb3-369cf65c33fd"
      },
      "source": [
        "from pprint import pprint\n",
        "\n",
        "list_of_fasttext_vectors = [\n",
        "  \"cc.bcl.300.vec\",\n",
        "  \"cc.ceb.300.vec\",\n",
        "  \"cc.en.300.vec\",\n",
        "  \"cc.ilo.300.vec\",\n",
        "  \"cc.pam.300.vec\",\n",
        "  \"cc.tl.300.vec\",\n",
        "  \"cc.war.300.vec\"\n",
        "]\n",
        "\n",
        "print(\"Loading words from vectors...\")\n",
        "words_per_lang = {}\n",
        "for f in list_of_fasttext_vectors:\n",
        "    fin = io.open(\"/content/\" + f, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "    data = []\n",
        "    for line in fin:\n",
        "        data.append(line.rstrip().split(' ')[0])\n",
        "    fin.close()\n",
        "    words_per_lang[f.split(\".\")[1]] = data\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading words from vectors...\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Moi8NUvWjmEt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81688f6c-d8ec-4743-fd18-d62353a743aa"
      },
      "source": [
        "print(\"Processing all words...\")\n",
        "all_words = set()\n",
        "for lang in [x.split(\".\")[1] for x in list_of_fasttext_vectors]:\n",
        "  print(\"--\", lang)\n",
        "  for word in words_per_lang[lang]:\n",
        "    all_words.add(word)\n",
        "print(\"Done.\")\n",
        "\n",
        "print(\"Total Unique Tokens:\", len(all_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing all words...\n",
            "-- bcl\n",
            "-- ceb\n",
            "-- en\n",
            "-- ilo\n",
            "-- pam\n",
            "-- tl\n",
            "-- war\n",
            "Done.\n",
            "Total Unique Tokens: 4909410\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b9GVMzLKITU",
        "colab_type": "text"
      },
      "source": [
        "# Token to Vector Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QO1bNxhMf0S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "121d6bbf-029d-4bd1-eefd-a44dd40ceb0a"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "statistics_all = []\n",
        "print(\"Total Tokens per Lang\")\n",
        "for lang in [x.split(\".\")[1] for x in list_of_fasttext_vectors]:\n",
        "  print(\"--- %s...\" % lang, end=\" \")\n",
        "\n",
        "  # Process Instagram\n",
        "  print(\"Instagram...\", end=\" \")\n",
        "  instagram_counter = Counter(instagram_tokens)\n",
        "  instagram_vocabulary = set([x for x in instagram_counter])\n",
        "  vocab_found = set.intersection(instagram_vocabulary, set(words_per_lang[lang]))\n",
        "  i_vocab_count = len(vocab_found)\n",
        "  i_token_count = 0\n",
        "  for token in vocab_found:\n",
        "    i_token_count += instagram_counter[token]\n",
        "  statistics_all.append({\n",
        "      'Language Code': lang,\n",
        "      'Platform': 'Instagram',\n",
        "      'Tokens Found': i_token_count,\n",
        "      'Vocab Found': i_vocab_count\n",
        "  })\n",
        "\n",
        "  # Process Twitter\n",
        "  print(\"Twitter...\", end=\" \")\n",
        "  twitter_counter = Counter(twitter_tokens)\n",
        "  twitter_vocabulary = set([x for x in twitter_counter])\n",
        "  vocab_found = set.intersection(twitter_vocabulary, set(words_per_lang[lang]))\n",
        "  t_vocab_count = len(vocab_found)\n",
        "  t_token_count = 0\n",
        "  for token in vocab_found:\n",
        "    t_token_count += twitter_counter[token]\n",
        "  statistics_all.append({\n",
        "      'Language Code': lang,\n",
        "      'Platform': 'Twitter',\n",
        "      'Tokens Found': t_token_count,\n",
        "      'Vocab Found': t_vocab_count\n",
        "  })\n",
        "\n",
        "  print(\"done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Tokens per Lang\n",
            "--- bcl... Instagram... Twitter... done.\n",
            "--- ceb... Instagram... Twitter... done.\n",
            "--- en... Instagram... Twitter... done.\n",
            "--- ilo... Instagram... Twitter... done.\n",
            "--- pam... Instagram... Twitter... done.\n",
            "--- tl... Instagram... Twitter... done.\n",
            "--- war... Instagram... Twitter... done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27l04GExMJue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "29ee79b7-b20f-4c7b-cab6-c685c9a38719"
      },
      "source": [
        "all_tokens_df = pd.DataFrame(statistics_all)\n",
        "print(all_tokens_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Language Code   Platform  Tokens Found  Vocab Found\n",
            "0            bcl  Instagram       1336388        10574\n",
            "1            bcl    Twitter      22786702        18608\n",
            "2            ceb  Instagram       1375462        10957\n",
            "3            ceb    Twitter      24108568        20762\n",
            "4             en  Instagram       1689032        44143\n",
            "5             en    Twitter      29699578       108766\n",
            "6            ilo  Instagram       1367715        12195\n",
            "7            ilo    Twitter      22772421        22728\n",
            "8            pam  Instagram       1438449        13997\n",
            "9            pam    Twitter      23860815        22196\n",
            "10            tl  Instagram       1691480        42533\n",
            "11            tl    Twitter      31231179       110893\n",
            "12           war  Instagram       1463918        16787\n",
            "13           war    Twitter      25103019        31574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VizEKSxnkNaO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "e24ce603-32a0-4af9-fa47-7f2c807801ea"
      },
      "source": [
        "statistics_unique = []\n",
        "print(\"Total Unique Words per Lang\")\n",
        "for lang in [x.split(\".\")[1] for x in list_of_fasttext_vectors]:\n",
        "  print(\"--- %s...\" % lang, end=\" \")\n",
        "\n",
        "  # Extract unique tokens only\n",
        "  total_words = len(words_per_lang[lang])\n",
        "  unique_words = set(words_per_lang[lang])\n",
        "  temp_l = [x.split(\".\")[1] for x in list_of_fasttext_vectors]\n",
        "  temp_l.remove(lang)\n",
        "  for lang2 in temp_l:\n",
        "    unique_words = unique_words - set(words_per_lang[lang2])\n",
        "  print(\"unique words extracted...\", end=\" \")\n",
        "\n",
        "  # Count tokens found\n",
        "  # Process Instagram\n",
        "  print(\"Instagram...\", end=\" \")\n",
        "  instagram_counter = Counter(instagram_tokens)\n",
        "  instagram_vocabulary = set([x for x in instagram_counter])\n",
        "  vocab_found = set.intersection(instagram_vocabulary, unique_words)\n",
        "  i_vocab_count = len(vocab_found)\n",
        "  i_token_count = 0\n",
        "  for token in vocab_found:\n",
        "    i_token_count += instagram_counter[token]\n",
        "  statistics_unique.append({\n",
        "      'Language Code': lang,\n",
        "      'Platform': 'Instagram',\n",
        "      'Tokens Found': i_token_count,\n",
        "      'Vocab Found': i_vocab_count\n",
        "  })\n",
        "\n",
        "  # Process Twitter\n",
        "  print(\"Twitter...\", end=\" \")\n",
        "  twitter_counter = Counter(twitter_tokens)\n",
        "  twitter_vocabulary = set([x for x in twitter_counter])\n",
        "  vocab_found = set.intersection(twitter_vocabulary, unique_words)\n",
        "  t_vocab_count = len(vocab_found)\n",
        "  t_token_count = 0\n",
        "  for token in vocab_found:\n",
        "    t_token_count += twitter_counter[token]\n",
        "  statistics_unique.append({\n",
        "      'Language Code': lang,\n",
        "      'Platform': 'Twitter',\n",
        "      'Tokens Found': t_token_count,\n",
        "      'Vocab Found': t_vocab_count\n",
        "  })\n",
        "\n",
        "  print(\"done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Unique Words per Lang\n",
            "--- bcl... unique words extracted... Instagram... Twitter... done.\n",
            "--- ceb... unique words extracted... Instagram... Twitter... done.\n",
            "--- en... unique words extracted... Instagram... Twitter... done.\n",
            "--- ilo... unique words extracted... Instagram... Twitter... done.\n",
            "--- pam... unique words extracted... Instagram... Twitter... done.\n",
            "--- tl... unique words extracted... Instagram... Twitter... done.\n",
            "--- war... unique words extracted... Instagram... Twitter... done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4MuslvvR9q_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "2cb337cd-76bb-49e6-8241-4c2947247111"
      },
      "source": [
        "unique_tokens_df = pd.DataFrame(statistics_unique)\n",
        "print(unique_tokens_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Language Code   Platform  Tokens Found  Vocab Found\n",
            "0            bcl  Instagram            83           62\n",
            "1            bcl    Twitter          6022         1346\n",
            "2            ceb  Instagram           346          210\n",
            "3            ceb    Twitter         48291         1994\n",
            "4             en  Instagram         30374        11782\n",
            "5             en    Twitter        457236        49961\n",
            "6            ilo  Instagram           159           97\n",
            "7            ilo    Twitter          9946         1465\n",
            "8            pam  Instagram           127           87\n",
            "9            pam    Twitter          4067          978\n",
            "10            tl  Instagram         23772         8810\n",
            "11            tl    Twitter       1332830        49731\n",
            "12           war  Instagram           350          218\n",
            "13           war    Twitter         12312         2200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rne_6jyDU14k",
        "colab_type": "text"
      },
      "source": [
        "# Misc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4-QujZzU8Vr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}